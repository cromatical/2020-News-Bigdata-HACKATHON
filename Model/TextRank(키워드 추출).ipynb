{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextRank\n",
    "\n",
    "참고 블로그 주소 : https://lovit.github.io/nlp/2019/04/30/textrank/\n",
    "\n",
    "2004년에 나온 통계기반 알고리즘, 따로 학습할 필요가 없어 빠르다! 글을 요약하기 위해서 개발됨. 핵심 문장, 단어를 뽑아낸다. 즉, 우리의 키워드 추출에 사용가능!\n",
    "\n",
    "아쉬운 점은 너무 오래된 기술이고, 딥러닝 기반 알고리즘에 비해 정확도가 낮다는점\n",
    "\n",
    "그런데 생각보다 잘 뽑힌다?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextRank based keyword extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextRank는 키워드 추출 기능과 핵심 문장 추출을 제공하는데, 우리는 그중에서 키워드 추출을 사용할것이다.\n",
    "\n",
    "키워드를 추출하기 위해 먼저 단어 그래프를 만들어야 한다.\n",
    "\n",
    "사용할 단어는 최소 빈도수(min_count) 이상 등장한 단어들을 사용한다.\n",
    "\n",
    "sents는 list of str 형식의 문장들\n",
    "\n",
    "tokenize 는 str 형식의 문장을 list of str 형식의 단어열로 나누는 토크나이저이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def scan_vocabulary(sents, tokenize, min_count=2):\n",
    "    counter = Counter(w for sent in sents for w in tokenize(sent))\n",
    "    counter = {w:c for w,c in counter.items() if c >= min_count}\n",
    "    idx_to_vocab = [w for w, _ in sorted(counter.items(), key=lambda x:-x[1])]\n",
    "    vocab_to_idx = {vocab:idx for idx, vocab in enumerate(idx_to_vocab)}\n",
    "    return idx_to_vocab, vocab_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextRank 에서 두 단어 간의 유사도를 정의하기 위해서는 두 단어의 co-occurrence 를 계산해야 한다.\n",
    "\n",
    "co-occurrence 는 문장 내에서 두 단어의 간격이 window 인 횟수이다.(window는 하이퍼 파라미터이며, 논문에서는 2~8 사이의 값을 이용하기를 추천함)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def cooccurrence(tokens, vocab_to_idx, window=2, min_cooccurrence=2):\n",
    "    counter = defaultdict(int)\n",
    "    for s, tokens_i in enumerate(tokens):\n",
    "        vocabs = [vocab_to_idx[w] for w in tokens_i if w in vocab_to_idx]\n",
    "        n = len(vocabs)\n",
    "        for i, v in enumerate(vocabs):\n",
    "            if window <= 0:\n",
    "                b, e = 0, n\n",
    "            else:\n",
    "                b = max(0, i - window)\n",
    "                e = min(i + window, n)\n",
    "            for j in range(b, e):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                counter[(v, vocabs[j])] += 1\n",
    "                counter[(vocabs[j], v)] += 1\n",
    "    counter = {k:v for k,v in counter.items() if v >= min_cooccurrence}\n",
    "    n_vocabs = len(vocab_to_idx)\n",
    "    return dict_to_mat(counter, n_vocabs, n_vocabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dict_to_mat 함수는 dict of dict 형식의 그래프를 scipy의 sparse matrix 로 변환하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def dict_to_mat(d, n_rows, n_cols):\n",
    "    rows, cols, data = [], [], []\n",
    "    for (i, j), v in d.items():\n",
    "        rows.append(i)\n",
    "        cols.append(j)\n",
    "        data.append(v)\n",
    "    return csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_graph(sents, tokenize=None, min_count=2, window=2, min_cooccurrence=2):\n",
    "    idx_to_vocab, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
    "    tokens = [tokenize(sent) for sent in sents]\n",
    "    g = cooccurrence(tokens, vocab_to_idx, window, min_cooccurrence, verbose)\n",
    "    return g, idx_to_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def pagerank(x, df=0.85, max_iter=30):\n",
    "    assert 0 < df < 1\n",
    "\n",
    "    # initialize\n",
    "    A = normalize(x, axis=0, norm='l1')\n",
    "    R = np.ones(A.shape[0]).reshape(-1,1)\n",
    "    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)\n",
    "\n",
    "    # iteration\n",
    "    for _ in range(max_iter):\n",
    "        R = df * (A * R) + bias\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank_keyword(sents, tokenize, min_count, window, min_cooccurrence, df=0.85, max_iter=30, topk=30):\n",
    "    g, idx_to_vocab = word_graph(sents, tokenize, min_count, window, min_cooccurrence)\n",
    "    R = pagerank(g, df, max_iter).reshape(-1)\n",
    "    idxs = R.argsort()[-topk:]\n",
    "    keywords = [(idx_to_vocab[idx], R[idx]) for idx in reversed(idxs)]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "\n",
    "komoran = Komoran()\n",
    "def komoran_tokenize(sent):\n",
    "    words = komoran.pos(sent, join=True)\n",
    "    words = [w for w in words if ('/NN' in w or '/XR' in w or '/VA' in w or '/VV' in w)]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __init__.py \n",
    "__name__ = 'textrank'\n",
    "__author__ = 'lovit'\n",
    "__version__ = '0.1.2'\n",
    "\n",
    "from .summarizer import KeywordSummarizer\n",
    "from .summarizer import KeysentenceSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank.py\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def pagerank(x, df=0.85, max_iter=30, bias=None):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    x : scipy.sparse.csr_matrix\n",
    "        shape = (n vertex, n vertex)\n",
    "    df : float\n",
    "        Damping factor, 0 < df < 1\n",
    "    max_iter : int\n",
    "        Maximum number of iteration\n",
    "    bias : numpy.ndarray or None\n",
    "        If None, equal bias\n",
    "    Returns\n",
    "    -------\n",
    "    R : numpy.ndarray\n",
    "        PageRank vector. shape = (n vertex, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    assert 0 < df < 1\n",
    "\n",
    "    # initialize\n",
    "    A = normalize(x, axis=0, norm='l1')\n",
    "    R = np.ones(A.shape[0]).reshape(-1,1)\n",
    "\n",
    "    # check bias\n",
    "    if bias is None:\n",
    "        bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)\n",
    "    else:\n",
    "        bias = bias.reshape(-1,1)\n",
    "        bias = A.shape[0] * bias / bias.sum()\n",
    "        assert bias.shape[0] == A.shape[0]\n",
    "        bias = (1 - df) * bias\n",
    "\n",
    "    # iteration\n",
    "    for _ in range(max_iter):\n",
    "        R = df * (A * R) + bias\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py \n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def scan_vocabulary(sents, tokenize=None, min_count=2):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    sents : list of str\n",
    "        Sentence list\n",
    "    tokenize : callable\n",
    "        tokenize(str) returns list of str\n",
    "    min_count : int\n",
    "        Minumum term frequency\n",
    "    Returns\n",
    "    -------\n",
    "    idx_to_vocab : list of str\n",
    "        Vocabulary list\n",
    "    vocab_to_idx : dict\n",
    "        Vocabulary to index mapper.\n",
    "    \"\"\"\n",
    "    counter = Counter(w for sent in sents for w in tokenize(sent))\n",
    "    counter = {w:c for w,c in counter.items() if c >= min_count}\n",
    "    idx_to_vocab = [w for w, _ in sorted(counter.items(), key=lambda x:-x[1])]\n",
    "    vocab_to_idx = {vocab:idx for idx, vocab in enumerate(idx_to_vocab)}\n",
    "    return idx_to_vocab, vocab_to_idx\n",
    "\n",
    "def tokenize_sents(sents, tokenize):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    sents : list of str\n",
    "        Sentence list\n",
    "    tokenize : callable\n",
    "        tokenize(sent) returns list of str (word sequence)\n",
    "    Returns\n",
    "    -------\n",
    "    tokenized sentence list : list of list of str\n",
    "    \"\"\"\n",
    "    return [tokenize(sent) for sent in sents]\n",
    "\n",
    "def vectorize(tokens, vocab_to_idx):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    tokens : list of list of str\n",
    "        Tokenzed sentence list\n",
    "    vocab_to_idx : dict\n",
    "        Vocabulary to index mapper\n",
    "    Returns\n",
    "    -------\n",
    "    sentence bow : scipy.sparse.csr_matrix\n",
    "        shape = (n_sents, n_terms)\n",
    "    \"\"\"\n",
    "    rows, cols, data = [], [], []\n",
    "    for i, tokens_i in enumerate(tokens):\n",
    "        for t, c in Counter(tokens_i).items():\n",
    "            j = vocab_to_idx.get(t, -1)\n",
    "            if j == -1:\n",
    "                continue\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            data.append(c)\n",
    "    n_sents = len(tokens)\n",
    "    n_terms = len(vocab_to_idx)\n",
    "    x = csr_matrix((data, (rows, cols)), shape=(n_sents, n_terms))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence.py\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# from .utils import scan_vocabulary\n",
    "# from .utils import tokenize_sents\n",
    "\n",
    "\n",
    "def sent_graph(sents, tokenize=None, min_count=2, min_sim=0.3,\n",
    "    similarity=None, vocab_to_idx=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    sents : list of str\n",
    "        Sentence list\n",
    "    tokenize : callable\n",
    "        tokenize(sent) return list of str\n",
    "    min_count : int\n",
    "        Minimum term frequency\n",
    "    min_sim : float\n",
    "        Minimum similarity between sentences\n",
    "    similarity : callable or str\n",
    "        similarity(s1, s2) returns float\n",
    "        s1 and s2 are list of str.\n",
    "        available similarity = [callable, 'cosine', 'textrank']\n",
    "    vocab_to_idx : dict\n",
    "        Vocabulary to index mapper.\n",
    "        If None, this function scan vocabulary first.\n",
    "    verbose : Boolean\n",
    "        If True, verbose mode on\n",
    "    Returns\n",
    "    -------\n",
    "    sentence similarity graph : scipy.sparse.csr_matrix\n",
    "        shape = (n sents, n sents)\n",
    "    \"\"\"\n",
    "\n",
    "    if vocab_to_idx is None:\n",
    "        idx_to_vocab, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
    "    else:\n",
    "        idx_to_vocab = [vocab for vocab, _ in sorted(vocab_to_idx.items(), key=lambda x:x[1])]\n",
    "\n",
    "    x = vectorize_sents(sents, tokenize, vocab_to_idx)\n",
    "    if similarity == 'cosine':\n",
    "        x = numpy_cosine_similarity_matrix(x, min_sim, verbose, batch_size=1000)\n",
    "    else:\n",
    "        x = numpy_textrank_similarity_matrix(x, min_sim, verbose, batch_size=1000)\n",
    "    return x\n",
    "\n",
    "def vectorize_sents(sents, tokenize, vocab_to_idx):\n",
    "    rows, cols, data = [], [], []\n",
    "    for i, sent in enumerate(sents):\n",
    "        counter = Counter(tokenize(sent))\n",
    "        for token, count in counter.items():\n",
    "            j = vocab_to_idx.get(token, -1)\n",
    "            if j == -1:\n",
    "                continue\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            data.append(count)\n",
    "    n_rows = len(sents)\n",
    "    n_cols = len(vocab_to_idx)\n",
    "    return csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n",
    "\n",
    "def numpy_cosine_similarity_matrix(x, min_sim=0.3, verbose=True, batch_size=1000):\n",
    "    n_rows = x.shape[0]\n",
    "    mat = []\n",
    "    for bidx in range(math.ceil(n_rows / batch_size)):\n",
    "        b = int(bidx * batch_size)\n",
    "        e = min(n_rows, int((bidx+1) * batch_size))\n",
    "        psim = 1 - pairwise_distances(x[b:e], x, metric='cosine')\n",
    "        rows, cols = np.where(psim >= min_sim)\n",
    "        data = psim[rows, cols]\n",
    "        mat.append(csr_matrix((data, (rows, cols)), shape=(e-b, n_rows)))\n",
    "        if verbose:\n",
    "            print('\\rcalculating cosine sentence similarity {} / {}'.format(b, n_rows), end='')\n",
    "    mat = sp.sparse.vstack(mat)\n",
    "    if verbose:\n",
    "        print('\\rcalculating cosine sentence similarity was done with {} sents'.format(n_rows))\n",
    "    return mat\n",
    "\n",
    "def numpy_textrank_similarity_matrix(x, min_sim=0.3, verbose=True, min_length=1, batch_size=1000):\n",
    "    n_rows, n_cols = x.shape\n",
    "\n",
    "    # Boolean matrix\n",
    "    rows, cols = x.nonzero()\n",
    "    data = np.ones(rows.shape[0])\n",
    "    z = csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n",
    "\n",
    "    # Inverse sentence length\n",
    "    size = np.asarray(x.sum(axis=1)).reshape(-1)\n",
    "    size[np.where(size <= min_length)] = 10000\n",
    "    size = np.log(size)\n",
    "\n",
    "    mat = []\n",
    "    for bidx in range(math.ceil(n_rows / batch_size)):\n",
    "\n",
    "        # slicing\n",
    "        b = int(bidx * batch_size)\n",
    "        e = min(n_rows, int((bidx+1) * batch_size))\n",
    "\n",
    "        # dot product\n",
    "        inner = z[b:e,:] * z.transpose()\n",
    "\n",
    "        # sentence len[i,j] = size[i] + size[j]\n",
    "        norm = size[b:e].reshape(-1,1) + size.reshape(1,-1)\n",
    "        norm = norm ** (-1)\n",
    "        norm[np.where(norm == np.inf)] = 0\n",
    "\n",
    "        # normalize\n",
    "        sim = inner.multiply(norm).tocsr()\n",
    "        rows, cols = (sim >= min_sim).nonzero()\n",
    "        data = np.asarray(sim[rows, cols]).reshape(-1)\n",
    "\n",
    "        # append\n",
    "        mat.append(csr_matrix((data, (rows, cols)), shape=(e-b, n_rows)))\n",
    "\n",
    "        if verbose:\n",
    "            print('\\rcalculating textrank sentence similarity {} / {}'.format(b, n_rows), end='')\n",
    "\n",
    "    mat = sp.sparse.vstack(mat)\n",
    "    if verbose:\n",
    "        print('\\rcalculating textrank sentence similarity was done with {} sents'.format(n_rows))\n",
    "\n",
    "    return mat\n",
    "\n",
    "def graph_with_python_sim(tokens, verbose, similarity, min_sim):\n",
    "    if similarity == 'cosine':\n",
    "        similarity = cosine_sent_sim\n",
    "    elif callable(similarity):\n",
    "        similarity = similarity\n",
    "    else:\n",
    "        similarity = textrank_sent_sim\n",
    "\n",
    "    rows, cols, data = [], [], []\n",
    "    n_sents = len(tokens)\n",
    "    for i, tokens_i in enumerate(tokens):\n",
    "        if verbose and i % 1000 == 0:\n",
    "            print('\\rconstructing sentence graph {} / {} ...'.format(i, n_sents), end='')\n",
    "        for j, tokens_j in enumerate(tokens):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            sim = similarity(tokens_i, tokens_j)\n",
    "            if sim < min_sim:\n",
    "                continue\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "            data.append(sim)\n",
    "    if verbose:\n",
    "        print('\\rconstructing sentence graph was constructed from {} sents'.format(n_sents))\n",
    "    return csr_matrix((data, (rows, cols)), shape=(n_sents, n_sents))\n",
    "\n",
    "def textrank_sent_sim(s1, s2):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    s1, s2 : list of str\n",
    "        Tokenized sentences\n",
    "    Returns\n",
    "    -------\n",
    "    Sentence similarity : float\n",
    "        Non-negative number\n",
    "    \"\"\"\n",
    "    n1 = len(s1)\n",
    "    n2 = len(s2)\n",
    "    if (n1 <= 1) or (n2 <= 1):\n",
    "        return 0\n",
    "    common = len(set(s1).intersection(set(s2)))\n",
    "    base = math.log(n1) + math.log(n2)\n",
    "    return common / base\n",
    "\n",
    "def cosine_sent_sim(s1, s2):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    s1, s2 : list of str\n",
    "        Tokenized sentences\n",
    "    Returns\n",
    "    -------\n",
    "    Sentence similarity : float\n",
    "        Non-negative number\n",
    "    \"\"\"\n",
    "    if (not s1) or (not s2):\n",
    "        return 0\n",
    "\n",
    "    s1 = Counter(s1)\n",
    "    s2 = Counter(s2)\n",
    "    norm1 = math.sqrt(sum(v ** 2 for v in s1.values()))\n",
    "    norm2 = math.sqrt(sum(v ** 2 for v in s2.values()))\n",
    "    prod = 0\n",
    "    for k, v in s1.items():\n",
    "        prod += v * s2.get(k, 0)\n",
    "    return prod / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarizer.py \n",
    "import numpy as np\n",
    "# from .rank import pagerank\n",
    "# from .sentence import sent_graph\n",
    "# from .word import word_graph\n",
    "\n",
    "\n",
    "class KeywordSummarizer:\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    sents : list of str\n",
    "        Sentence list\n",
    "    tokenize : callable\n",
    "        Tokenize function: tokenize(str) = list of str\n",
    "    min_count : int\n",
    "        Minumum frequency of words will be used to construct sentence graph\n",
    "    window : int\n",
    "        Word cooccurrence window size. Default is -1.\n",
    "        '-1' means there is cooccurrence between two words if the words occur in a sentence\n",
    "    min_cooccurrence : int\n",
    "        Minimum cooccurrence frequency of two words\n",
    "    vocab_to_idx : dict or None\n",
    "        Vocabulary to index mapper\n",
    "    df : float\n",
    "        PageRank damping factor\n",
    "    max_iter : int\n",
    "        Number of PageRank iterations\n",
    "    verbose : Boolean\n",
    "        If True, it shows training progress\n",
    "    \"\"\"\n",
    "    def __init__(self, sents=None, tokenize=None, min_count=2,\n",
    "        window=-1, min_cooccurrence=2, vocab_to_idx=None,\n",
    "        df=0.85, max_iter=30, verbose=False):\n",
    "\n",
    "        self.tokenize = tokenize\n",
    "        self.min_count = min_count\n",
    "        self.window = window\n",
    "        self.min_cooccurrence = min_cooccurrence\n",
    "        self.vocab_to_idx = vocab_to_idx\n",
    "        self.df = df\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if sents is not None:\n",
    "            self.train_textrank(sents)\n",
    "\n",
    "    def train_textrank(self, sents, bias=None):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        sents : list of str\n",
    "            Sentence list\n",
    "        bias : None or numpy.ndarray\n",
    "            PageRank bias term\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        g, self.idx_to_vocab = word_graph(sents,\n",
    "            self.tokenize, self.min_count,self.window,\n",
    "            self.min_cooccurrence, self.vocab_to_idx, self.verbose)\n",
    "        self.R = pagerank(g, self.df, self.max_iter, bias).reshape(-1)\n",
    "        if self.verbose:\n",
    "            print('trained TextRank. n words = {}'.format(self.R.shape[0]))\n",
    "\n",
    "    def keywords(self, topk=30):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        topk : int\n",
    "            Number of keywords selected from TextRank\n",
    "        Returns\n",
    "        -------\n",
    "        keywords : list of tuple\n",
    "            Each tuple stands for (word, rank)\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'R'):\n",
    "            raise RuntimeError('Train textrank first or use summarize function')\n",
    "        idxs = self.R.argsort()[-topk:]\n",
    "        keywords = [(self.idx_to_vocab[idx], self.R[idx]) for idx in reversed(idxs)]\n",
    "        return keywords\n",
    "\n",
    "    def summarize(self, sents, topk=30):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        sents : list of str\n",
    "            Sentence list\n",
    "        topk : int\n",
    "            Number of keywords selected from TextRank\n",
    "        Returns\n",
    "        -------\n",
    "        keywords : list of tuple\n",
    "            Each tuple stands for (word, rank)\n",
    "        \"\"\"\n",
    "        self.train_textrank(sents)\n",
    "        return self.keywords(topk)\n",
    "\n",
    "\n",
    "class KeysentenceSummarizer:\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    sents : list of str\n",
    "        Sentence list\n",
    "    tokenize : callable\n",
    "        Tokenize function: tokenize(str) = list of str\n",
    "    min_count : int\n",
    "        Minumum frequency of words will be used to construct sentence graph\n",
    "    min_sim : float\n",
    "        Minimum similarity between sentences in sentence graph\n",
    "    similarity : str\n",
    "        available similarity = ['cosine', 'textrank']\n",
    "    vocab_to_idx : dict or None\n",
    "        Vocabulary to index mapper\n",
    "    df : float\n",
    "        PageRank damping factor\n",
    "    max_iter : int\n",
    "        Number of PageRank iterations\n",
    "    verbose : Boolean\n",
    "        If True, it shows training progress\n",
    "    \"\"\"\n",
    "    def __init__(self, sents=None, tokenize=None, min_count=2,\n",
    "        min_sim=0.3, similarity=None, vocab_to_idx=None,\n",
    "        df=0.85, max_iter=30, verbose=False):\n",
    "\n",
    "        self.tokenize = tokenize\n",
    "        self.min_count = min_count\n",
    "        self.min_sim = min_sim\n",
    "        self.similarity = similarity\n",
    "        self.vocab_to_idx = vocab_to_idx\n",
    "        self.df = df\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if sents is not None:\n",
    "            self.train_textrank(sents)\n",
    "\n",
    "    def train_textrank(self, sents, bias=None):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        sents : list of str\n",
    "            Sentence list\n",
    "        bias : None or numpy.ndarray\n",
    "            PageRank bias term\n",
    "            Shape must be (n_sents,)\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        g = sent_graph(sents, self.tokenize, self.min_count,\n",
    "            self.min_sim, self.similarity, self.vocab_to_idx, self.verbose)\n",
    "        self.R = pagerank(g, self.df, self.max_iter, bias).reshape(-1)\n",
    "        if self.verbose:\n",
    "            print('trained TextRank. n sentences = {}'.format(self.R.shape[0]))\n",
    "\n",
    "    def summarize(self, sents, topk=30, bias=None):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        sents : list of str\n",
    "            Sentence list\n",
    "        topk : int\n",
    "            Number of key-sentences to be selected.\n",
    "        bias : None or numpy.ndarray\n",
    "            PageRank bias term\n",
    "            Shape must be (n_sents,)\n",
    "        Returns\n",
    "        -------\n",
    "        keysents : list of tuple\n",
    "            Each tuple stands for (sentence index, rank, sentence)\n",
    "        Usage\n",
    "        -----\n",
    "            >>> from textrank import KeysentenceSummarizer\n",
    "            >>> summarizer = KeysentenceSummarizer(tokenize = tokenizer, min_sim = 0.5)\n",
    "            >>> keysents = summarizer.summarize(texts, topk=30)\n",
    "        \"\"\"\n",
    "        n_sents = len(sents)\n",
    "        if isinstance(bias, np.ndarray):\n",
    "            if bias.shape != (n_sents,):\n",
    "                raise ValueError('The shape of bias must be (n_sents,) but {}'.format(bias.shape))\n",
    "        elif bias is not None:\n",
    "            raise ValueError('The type of bias must be None or numpy.ndarray but the type is {}'.format(type(bias)))\n",
    "\n",
    "        self.train_textrank(sents, bias)\n",
    "        idxs = self.R.argsort()[-topk:]\n",
    "        keysents = [(idx, self.R[idx], sents[idx]) for idx in reversed(idxs)]\n",
    "        return keysents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word.py\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# from .utils import scan_vocabulary\n",
    "# from .utils import tokenize_sents\n",
    "\n",
    "\n",
    "def word_graph(sents, tokenize=None, min_count=2, window=2,\n",
    "    min_cooccurrence=2, vocab_to_idx=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    sents : list of str\n",
    "        Sentence list\n",
    "    tokenize : callable\n",
    "        tokenize(str) returns list of str\n",
    "    min_count : int\n",
    "        Minumum term frequency\n",
    "    window : int\n",
    "        Co-occurrence window size\n",
    "    min_cooccurrence : int\n",
    "        Minimum cooccurrence frequency\n",
    "    vocab_to_idx : dict\n",
    "        Vocabulary to index mapper.\n",
    "        If None, this function scan vocabulary first.\n",
    "    verbose : Boolean\n",
    "        If True, verbose mode on\n",
    "    Returns\n",
    "    -------\n",
    "    co-occurrence word graph : scipy.sparse.csr_matrix\n",
    "    idx_to_vocab : list of str\n",
    "        Word list corresponding row and column\n",
    "    \"\"\"\n",
    "    if vocab_to_idx is None:\n",
    "        idx_to_vocab, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
    "    else:\n",
    "        idx_to_vocab = [vocab for vocab, _ in sorted(vocab_to_idx.items(), key=lambda x:x[1])]\n",
    "\n",
    "    tokens = tokenize_sents(sents, tokenize)\n",
    "    g = cooccurrence(tokens, vocab_to_idx, window, min_cooccurrence, verbose)\n",
    "    return g, idx_to_vocab\n",
    "\n",
    "def cooccurrence(tokens, vocab_to_idx, window=2, min_cooccurrence=2, verbose=False):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    tokens : list of list of str\n",
    "        Tokenized sentence list\n",
    "    vocab_to_idx : dict\n",
    "        Vocabulary to index mapper\n",
    "    window : int\n",
    "        Co-occurrence window size\n",
    "    min_cooccurrence : int\n",
    "        Minimum cooccurrence frequency\n",
    "    verbose : Boolean\n",
    "        If True, verbose mode on\n",
    "    Returns\n",
    "    -------\n",
    "    co-occurrence matrix : scipy.sparse.csr_matrix\n",
    "        shape = (n_vocabs, n_vocabs)\n",
    "    \"\"\"\n",
    "    counter = defaultdict(int)\n",
    "    for s, tokens_i in enumerate(tokens):\n",
    "        if verbose and s % 1000 == 0:\n",
    "            print('\\rword cooccurrence counting {}'.format(s), end='')\n",
    "        vocabs = [vocab_to_idx[w] for w in tokens_i if w in vocab_to_idx]\n",
    "        n = len(vocabs)\n",
    "        for i, v in enumerate(vocabs):\n",
    "            if window <= 0:\n",
    "                b, e = 0, n\n",
    "            else:\n",
    "                b = max(0, i - window)\n",
    "                e = min(i + window, n)\n",
    "            for j in range(b, e):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                counter[(v, vocabs[j])] += 1\n",
    "                counter[(vocabs[j], v)] += 1\n",
    "    counter = {k:v for k,v in counter.items() if v >= min_cooccurrence}\n",
    "    n_vocabs = len(vocab_to_idx)\n",
    "    if verbose:\n",
    "        print('\\rword cooccurrence counting from {} sents was done'.format(s+1))\n",
    "    return dict_to_mat(counter, n_vocabs, n_vocabs)\n",
    "\n",
    "def dict_to_mat(d, n_rows, n_cols):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    d : dict\n",
    "        key : (i,j) tuple\n",
    "        value : float value\n",
    "    Returns\n",
    "    -------\n",
    "    scipy.sparse.csr_matrix\n",
    "    \"\"\"\n",
    "    rows, cols, data = [], [], []\n",
    "    for (i, j), v in d.items():\n",
    "        rows.append(i)\n",
    "        cols.append(j)\n",
    "        data.append(v)\n",
    "    return csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open 에 원하는 txet를 넣으면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('qqqq.txt', encoding='utf-8') as f:\n",
    "    sents = [sent.strip() for sent in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "국회/NNG (2.14)\n",
      "대통령/NNG (2.09)\n",
      "문/NNG (1.79)\n",
      "대책/NNG (1.51)\n",
      "공수/NNP (1.24)\n",
      "처/NNG (1.24)\n",
      "말/NNG (1.15)\n",
      "주택/NNG (1.12)\n",
      "뉴딜/NNP (1.0)\n",
      "부동산/NNP (0.919)\n",
      "일/NNG (0.873)\n",
      "출범/NNG (0.873)\n",
      "것/NNB (0.851)\n",
      "정부/NNG (0.829)\n",
      "대/NNB (0.802)\n",
      "부담/NNG (0.78)\n",
      "연설/NNG (0.723)\n",
      "일/NNB (0.723)\n",
      "한국/NNP (0.716)\n",
      "판/NNB (0.716)\n",
      "안정/NNG (0.624)\n",
      "투기/NNG (0.572)\n",
      "돈/NNG (0.572)\n",
      "벌/NNB (0.572)\n",
      "수/NNB (0.572)\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 작업\n",
    "\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "komoran = Komoran()\n",
    "def komoran_tokenize(sent):\n",
    "    words = komoran.pos(sent, join=True)\n",
    "    words = [w for w in words if ('/NN' in w)]\n",
    "    return words\n",
    "\n",
    "keyword_extractor = KeywordSummarizer(\n",
    "    tokenize = komoran_tokenize,\n",
    "    window = -1,\n",
    "    verbose = False\n",
    ")\n",
    "keywords = keyword_extractor.summarize(sents, topk=30)\n",
    "for word, rank in keywords:\n",
    "    print('{} ({:.3})'.format(word, rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
